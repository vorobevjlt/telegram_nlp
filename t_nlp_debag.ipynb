{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T23:17:08.624157Z",
     "start_time": "2025-11-06T23:17:05.932093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
    "import mlxtend\n",
    "from mlxtend.evaluate import paired_ttest_kfold_cv\n",
    "from plotly.offline import iplot\n",
    "import cufflinks as cf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "%matplotlib inline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ],
   "id": "439def734890d227",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/joy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.1.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T23:17:08.660210Z",
     "start_time": "2025-11-06T23:17:08.658104Z"
    }
   },
   "cell_type": "code",
   "source": "RND_STATE = 73",
   "id": "728f72325ece0e30",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:56:06.552252Z",
     "start_time": "2025-11-05T15:56:05.548234Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(\"/data/4000_per_month_article.csv\")",
   "id": "4d225c87b1df6ec7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:56:08.125698Z",
     "start_time": "2025-11-05T15:56:08.112267Z"
    }
   },
   "cell_type": "code",
   "source": "df.dropna(inplace=True)",
   "id": "35b4613655fba237",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:56:08.575147Z",
     "start_time": "2025-11-05T15:56:08.564960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cats = ['Деньги', 'Свое дело', 'Авто', 'Недвижимость']\n",
    "df.drop(df[df['category'].isin(cats)].index, inplace=True)"
   ],
   "id": "ee7bac66bc40e934",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T23:17:11.516952Z",
     "start_time": "2025-11-06T23:17:11.512232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_prep(text) -> str:\n",
    "    segmenter = Segmenter()\n",
    "    morph_vocab = MorphVocab()\n",
    "    emb = NewsEmbedding()\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    stop_words = stopwords.words('russian')\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "\n",
    "    lemmas = [_.lemma for _ in doc.tokens]\n",
    "    words = [lemma for lemma in lemmas if lemma.isalpha() and len(lemma) > 2]\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ],
   "id": "759b0df69bea2624",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T02:18:05.514198Z",
     "start_time": "2025-11-06T02:18:05.380556Z"
    }
   },
   "cell_type": "code",
   "source": "df['tokenised_text'] = df.text.apply(text_prep)\n",
   "id": "46a1878efcb83554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joy/ml_sandbox/t_nlp/tests\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T02:22:55.628910Z",
     "start_time": "2025-11-06T02:22:53.798985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dft = pd.read_csv('tests/4000_per_month_article_tokens.csv')\n",
    "dft.dropna(inplace=True)\n",
    "dft.columns"
   ],
   "id": "ce88628338a4bfaa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'tags', 'category', 'text', 'tokenised_text'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:11:48.168360Z",
     "start_time": "2025-11-06T12:11:47.550391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dft.tokenised_text.str.split(),\n",
    "                                                    dft.category.values,\n",
    "                                                    random_state=RND_STATE)"
   ],
   "id": "41f1a0418dc6f0b5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:11:22.496849Z",
     "start_time": "2025-11-06T12:11:22.348541Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3dfa82d8209f287e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:12:00.190628Z",
     "start_time": "2025-11-06T12:11:51.575657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Word2Vec(sentences=X_train,\n",
    "                 vector_size=200,\n",
    "                 min_count=10,\n",
    "                 window=2,\n",
    "                 seed=RND_STATE)"
   ],
   "id": "9c28aa3961d3f3a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:13:31.712986Z",
     "start_time": "2025-11-06T12:13:31.691728Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.most_similar(positive=[\"родина\"])",
   "id": "c3a532ec591e4222",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('белорус', 0.6354150772094727),\n",
       " ('кобахидзе', 0.6200356483459473),\n",
       " ('грузинский', 0.6056091785430908),\n",
       " ('курец', 0.5962410569190979),\n",
       " ('мечта', 0.5945427417755127),\n",
       " ('сдпг', 0.5923218131065369),\n",
       " ('опзж', 0.5920796394348145),\n",
       " ('социалист', 0.5899027585983276),\n",
       " ('коммунист', 0.5895706415176392),\n",
       " ('провозгласить', 0.5887891054153442)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T23:17:30.872577Z",
     "start_time": "2025-11-06T23:17:30.868922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    \"\"\"Get mean of vectors\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.word2vec = model.wv\n",
    "        self.dim = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.get_vector(w)\n",
    "                for w in words if w in self.word2vec] or\n",
    "                [np.zeros(self.dim)], axis=0)\n",
    "            for words in X])"
   ],
   "id": "2b6f69b99577a204",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:16:02.151987Z",
     "start_time": "2025-11-06T12:16:02.143396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "pipe = Pipeline([('w2v', MeanEmbeddingVectorizer(model)),\n",
    "                 ('clf', RandomForestClassifier(random_state=RND_STATE))])"
   ],
   "id": "dc49e6dfb2c6239a",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:16:28.317568Z",
     "start_time": "2025-11-06T12:16:03.511245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "joblib.dump(pipe, 'models/4000_doc.pkl')"
   ],
   "id": "2b71a0ea0f82c5e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4000_doc.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T23:20:35.319200Z",
     "start_time": "2025-11-06T23:20:35.229256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "# make sure W2VMeanVectorizer class is defined/importable before this line\n",
    "pipe = joblib.load(\"/models/4000_doc.pkl\")"
   ],
   "id": "2b75d922d75983f3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:17:02.697886Z",
     "start_time": "2025-11-06T12:17:01.394857Z"
    }
   },
   "cell_type": "code",
   "source": "print(classification_report(y_test, pipe.predict(X_test)))",
   "id": "64f56e472627a3c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            Бизнес       0.65      0.60      0.62       405\n",
      "          Общество       0.84      0.91      0.87      2971\n",
      "          Политика       0.85      0.84      0.84      1941\n",
      "Технологии и медиа       0.73      0.53      0.62       244\n",
      "           Финансы       0.80      0.67      0.73       159\n",
      "         Экономика       0.54      0.29      0.38       189\n",
      "\n",
      "          accuracy                           0.82      5909\n",
      "         macro avg       0.73      0.64      0.68      5909\n",
      "      weighted avg       0.81      0.82      0.82      5909\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:52:48.828604Z",
     "start_time": "2025-11-06T12:52:48.803398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %pip install selenium\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from IPython import display\n",
    "from urllib.parse import urlencode\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "class RBCparser:\n",
    "    def __init__(self,\n",
    "                 query='РБК',\n",
    "                 project='rbcnews',\n",
    "                 category='TopRbcRu_economics',\n",
    "                 material='',\n",
    "                 dateFrom='2025-01-01',\n",
    "                 dateTo='2025-01-01',\n",
    "                 page=0):\n",
    "\n",
    "        self.query = query\n",
    "        self.project = project\n",
    "        self.category = category\n",
    "        self.material = material\n",
    "        self.dateFrom = dateFrom\n",
    "        self.dateTo = dateTo\n",
    "        self.page = page\n",
    "\n",
    "        self.param_dict = {\n",
    "            'query': query,\n",
    "            'project': project,\n",
    "            'category': category,\n",
    "            'dateFrom': datetime.strptime(dateFrom, '%Y-%m-%d').strftime('%d.%m.%Y'),\n",
    "            'dateTo': datetime.strptime(dateTo, '%Y-%m-%d').strftime('%d.%m.%Y'),\n",
    "            'page': str(page),\n",
    "            'material': material\n",
    "        }\n",
    "    @staticmethod\n",
    "    def _get_url(parameters: dict) -> str:\n",
    "        base_url = \"https://www.rbc.ru/search/ajax/\"\n",
    "        return base_url + \"?\" + urlencode(parameters, encoding=\"utf-8\")\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_article(session, url, ar_date, ar_category):\n",
    "        r_page = session.get(url)\n",
    "        soup = bs(r_page.text, \"html.parser\")\n",
    "\n",
    "        container_text = soup.find('div', class_='article__text article__text_free')\n",
    "        ar_text = \" \".join(\n",
    "            p.get_text(strip=True) for p in container_text.find_all('p')\n",
    "        ) if container_text else \"\"\n",
    "\n",
    "        container_tags = soup.find('div', class_='article__tags__container')\n",
    "        ar_tags = \". \".join(\n",
    "            tag.get_text(strip=True) for tag in container_tags.find_all('a', class_='article__tags__item')\n",
    "        ) if container_tags else \"\"\n",
    "\n",
    "        return {\n",
    "            'date': ar_date,\n",
    "            'tags': ar_tags,\n",
    "            'category': ar_category,\n",
    "            'text': ar_text\n",
    "        }\n",
    "\n",
    "    def _get_data(self, max_articles=2000):\n",
    "        res = []\n",
    "        session = rq.Session()\n",
    "        page = 0\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            while len(res) < max_articles:\n",
    "                d = {**self.param_dict, \"page\": str(page)}\n",
    "                try:\n",
    "                    response = session.get(self._get_url(d))\n",
    "                    response.raise_for_status()\n",
    "                    items = response.json().get('items', [])\n",
    "                except Exception as e:\n",
    "                    print(f\"[Ошибка на странице {page}]: {e}\")\n",
    "                    break\n",
    "\n",
    "                if not items:\n",
    "                    break\n",
    "\n",
    "                futures = [\n",
    "                    executor.submit(\n",
    "                        self.fetch_article,\n",
    "                        session,\n",
    "                        item.get('fronturl'),\n",
    "                        item.get('publish_date'),\n",
    "                        item.get('category')\n",
    "                    )\n",
    "                    for item in items\n",
    "                ]\n",
    "\n",
    "                for f in futures:\n",
    "                    res.append(f.result())\n",
    "\n",
    "                page += 1\n",
    "                if len(res) >= max_articles:\n",
    "                    break\n",
    "\n",
    "        return pd.DataFrame(res[:max_articles])\n",
    "\n",
    "    def get_range_data(self, save_csv=False, csv_name=\"default_name.csv\", max_articles_per_month=2000):\n",
    "\n",
    "        start = datetime.strptime(self.dateFrom, \"%Y-%m-%d\")\n",
    "        end = datetime.strptime(self.dateTo, \"%Y-%m-%d\")\n",
    "\n",
    "        all_dfs = []\n",
    "\n",
    "        while start <= end:\n",
    "            month_start = start.replace(day=1)\n",
    "            month_end = (month_start + relativedelta(months=1)) - relativedelta(days=1)\n",
    "\n",
    "            if month_end > end:\n",
    "                month_end = end\n",
    "\n",
    "            print(f\"Статьи за {month_start.strftime('%Y-%m')}\")\n",
    "\n",
    "            parser = RBCparser(\n",
    "                query=self.query,\n",
    "                project=self.project,\n",
    "                category=self.category,\n",
    "                material=self.material,\n",
    "                dateFrom=month_start.strftime(\"%Y-%m-%d\"),\n",
    "                dateTo=month_end.strftime(\"%Y-%m-%d\")\n",
    "            )\n",
    "\n",
    "            df_month = parser._get_data(max_articles=max_articles_per_month)\n",
    "            if not df_month.empty:\n",
    "                all_dfs.append(df_month)\n",
    "\n",
    "            start = month_start + relativedelta(months=1)\n",
    "\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "        if save_csv:\n",
    "            final_df.to_csv(csv_name, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        return final_df\n"
   ],
   "id": "a90b87fba5779527",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:55:08.481335Z",
     "start_time": "2025-11-06T12:55:08.471472Z"
    }
   },
   "cell_type": "code",
   "source": "p = RBCparser(dateFrom = '2020-01-01', dateTo = '2020-01-20')",
   "id": "a312b3ab3fdcc108",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:55:27.938125Z",
     "start_time": "2025-11-06T12:55:11.021931Z"
    }
   },
   "cell_type": "code",
   "source": "test_df = p.get_range_data(max_articles_per_month=200)",
   "id": "20b9896869e75ffc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статьи за 2020-01\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T13:22:20.180022Z",
     "start_time": "2025-11-06T13:22:20.168816Z"
    }
   },
   "cell_type": "code",
   "source": "test_df.dropna(inplace=True)",
   "id": "87abfd3d5c43c86c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T13:23:30.272294Z",
     "start_time": "2025-11-06T13:22:26.923056Z"
    }
   },
   "cell_type": "code",
   "source": "test_df['tokenise_text'] = test_df.text.apply(text_prep)",
   "id": "42e51daa8ddee3e9",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T13:23:44.460156Z",
     "start_time": "2025-11-06T13:23:44.456805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = test_df['tokenise_text']\n",
    "y = test_df['category']"
   ],
   "id": "ec80cdf0679269f5",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T13:23:52.833905Z",
     "start_time": "2025-11-06T13:23:52.649177Z"
    }
   },
   "cell_type": "code",
   "source": "print(classification_report(y, pipe.predict(X)))",
   "id": "44674fe3fa04c54a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            Бизнес       0.00      0.00      0.00        13\n",
      "          Общество       0.53      1.00      0.69       105\n",
      "          Политика       0.00      0.00      0.00        58\n",
      "Технологии и медиа       0.00      0.00      0.00         5\n",
      "           Финансы       0.00      0.00      0.00         9\n",
      "         Экономика       0.00      0.00      0.00         9\n",
      "\n",
      "          accuracy                           0.53       199\n",
      "         macro avg       0.09      0.17      0.12       199\n",
      "      weighted avg       0.28      0.53      0.36       199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joy/ml_sandbox/t_nlp/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/joy/ml_sandbox/t_nlp/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/joy/ml_sandbox/t_nlp/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1c4bcedcb7b2478d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "68e6ce33ac82de8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
